{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(6, 16, 5, 1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(16, 120, 5, 1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc6 = nn.Linear(120, 10)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.maxpool2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.maxpool4(x))\n",
    "        x = self.relu5(self.conv5(x)).squeeze()\n",
    "        x = self.relu6(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sd_card_files/params.bin', 'rb') as f:\n",
    "    params = np.fromfile(f, dtype=np.float32) \n",
    "    conv1_weights = torch.from_numpy(params[:150].reshape(6, 1, 5, 5))\n",
    "    conv1_bias = torch.from_numpy(params[150:156].reshape(6))\n",
    "    conv3_weights = torch.from_numpy(params[156:156+2400].reshape(16, 6, 5, 5))\n",
    "    conv3_bias = torch.from_numpy(params[2556:2572].reshape(16))\n",
    "    conv5_weights = torch.from_numpy(params[2572:50572].reshape(120, 16, 5, 5))\n",
    "    conv5_bias = torch.from_numpy(params[50572:50692].reshape(120))\n",
    "    fc6_weights = torch.from_numpy(params[50692:51892].reshape(10, 120))\n",
    "    fc6_bias = torch.from_numpy(params[51892:].reshape(10))\n",
    "\n",
    "with open('sd_card_files/images.bin', 'rb') as f:\n",
    "    images_raw = np.fromfile(f, dtype=np.uint8)\n",
    "    images_raw = images_raw[16:].reshape(-1, 1, 28, 28)\n",
    "    images = np.ones((images_raw.shape[0], 1, 32, 32)) * -1\n",
    "    images[:, :, 2:30, 2:30] = images_raw / 255.0 * 2.0 - 1.0\n",
    "    images = torch.from_numpy(images).float()\n",
    "\n",
    "with open('sd_card_files/labels.bin', 'rb') as f:\n",
    "    labels = np.fromfile(f, dtype=np.uint8)\n",
    "    labels = torch.from_numpy(labels[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet()\n",
    "lenet.conv1.weight.data = conv1_weights\n",
    "lenet.conv1.bias.data = conv1_bias\n",
    "lenet.conv3.weight.data = conv3_weights\n",
    "lenet.conv3.bias.data = conv3_bias\n",
    "lenet.conv5.weight.data = conv5_weights\n",
    "lenet.conv5.bias.data = conv5_bias\n",
    "lenet.fc6.weight.data = fc6_weights\n",
    "lenet.fc6.bias.data = fc6_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[225, :, :].squeeze())\n",
    "torch.argmax(lenet(images[225:226, :, :, :]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(lenet(images.reshape(-1, 1, 32, 32)), dim=-1).to(dtype=torch.uint8)\n",
    "torch.sum(pred == labels) / pred.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.conv1.weight.requires_grad = False\n",
    "lenet.conv1.bias.requires_grad = False\n",
    "lenet.conv3.weight.requires_grad = False\n",
    "lenet.conv3.bias.requires_grad = False\n",
    "lenet.fc6.weight.requires_grad = False\n",
    "lenet.fc6.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer:\n",
    "    def __init__(self, nbits=8) -> None:\n",
    "        self.nbits = nbits\n",
    "\n",
    "    def channel_quantize(self, weights):\n",
    "        maximum = np.max(weights, axis=-1)\n",
    "        minimum = np.min(weights, axis=-1)\n",
    "        scale = (maximum - minimum) / (2 ** self.nbits - 1)\n",
    "        scale = np.repeat(scale, weights.shape[-1], axis=-1).reshape(*scale.shape, weights.shape[-1])\n",
    "        bias = minimum\n",
    "        bias = np.repeat(bias, weights.shape[-1], axis=-1).reshape(*bias.shape, weights.shape[-1])\n",
    "        t = ((weights - bias) / scale).round()\n",
    "\n",
    "        t = t * scale + bias\n",
    "\n",
    "        return t, scale, bias\n",
    "    \n",
    "    def tensor_quantize(self, weights):\n",
    "        maximum = np.max(weights)\n",
    "        minimum = np.min(weights)\n",
    "        scale = (maximum - minimum) / (2 ** self.nbits - 1)\n",
    "        bias = minimum\n",
    "        t = ((weights - bias) / scale).round()\n",
    "\n",
    "        t = t * scale + bias\n",
    "\n",
    "        return t, scale, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = Quantizer(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, scale, bias = quantizer.channel_quantize(lenet.conv5.weight.data.numpy().reshape(120, 16, 25))\n",
    "t = t.reshape(120, 16, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.conv5.weight.data = torch.from_numpy(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(lenet(images.reshape(-1, 1, 32, 32)), dim=-1).to(dtype=torch.uint8)\n",
    "torch.sum(pred == labels) / pred.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_w5 = ((t.reshape(120, 16, 25) - bias) / scale).round().reshape(120, 16, 5, 5).astype(np.uint8)\n",
    "quantized_w5.tofile(\"quantized_w5.bin\")\n",
    "\n",
    "scale = scale[:, :, 0]\n",
    "bias = bias[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale.tofile(\"scale.bin\")\n",
    "bias.tofile(\"bias.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quantized_w5.bin', 'rb') as f:\n",
    "    params = np.fromfile(f, dtype=np.uint8) \n",
    "    read_from_file_quantized_w5 = params.reshape(120, 16, 5, 5)\n",
    "with open('scale.bin', 'rb') as f:\n",
    "    params = np.fromfile(f, dtype=np.float32) \n",
    "    read_from_file_scale = params.reshape(120, 16)\n",
    "with open('bias.bin', 'rb') as f:\n",
    "    params = np.fromfile(f, dtype=np.float32) \n",
    "    read_from_file_bias = params.reshape(120, 16)\n",
    "\n",
    "\n",
    "print(np.sum(quantized_w5 == read_from_file_quantized_w5), np.sum(read_from_file_scale == scale), np.sum(read_from_file_bias == bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.repeat(scale, 25, axis=-1).reshape(*scale.shape, 25)\n",
    "bias = np.repeat(bias, 25, axis=-1).reshape(*bias.shape, 25)\n",
    "dequantized_w5 = (quantized_w5.reshape(120, 16, 25) * scale + bias).reshape(120, 16, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.conv5.weight.data = torch.from_numpy(dequantized_w5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(lenet(images.reshape(-1, 1, 32, 32)), dim=-1).to(dtype=torch.uint8)\n",
    "torch.sum(pred == labels) / pred.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE549",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
